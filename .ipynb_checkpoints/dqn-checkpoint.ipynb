{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning Cartpole\n",
    "\n",
    "Using what I learned in Lazy Programmer's Udemy class Deep Reinforcement Learning in Python, I'm going to try and create my own DQN that can solve OpenAI's Cartpole problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create DQN class.\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, state_num, action_num):\n",
    "        self.action_num = action_num\n",
    "        self.gamma = 0.95 # Discount rate.\n",
    "        self.batch = 64 # Number of replayed experiences batched together.\n",
    "        self.experiences = deque(maxlen = 2000)\n",
    "        \n",
    "        # Create Neural Network that will act as a function\n",
    "        # approximator for Q[s,a].\n",
    "        self.model = Sequential()\n",
    "        # First dense layer with relu activation.\n",
    "        self.model.add(Dense(24, input_dim=state_num))\n",
    "        self.model.add(Activation('relu'))\n",
    "        # Second dense layer with relu activation.\n",
    "        self.model.add(Dense(24))\n",
    "        self.model.add(Activation('relu'))\n",
    "        # Output layer with linear activation.\n",
    "        self.model.add(Dense(self.action_num))\n",
    "        self.model.add(Activation('linear'))\n",
    "        # Loss function.\n",
    "        self.model.compile(loss = 'mse', optimizer=Adam(lr = 0.001),\n",
    "              metrics=['categorical_accuracy'])\n",
    "        \n",
    "        self.model.summary()\n",
    "        \n",
    "    def ExploitAction(self, state):\n",
    "        action_values = self.model.predict(state)\n",
    "        exploit_action_index = np.argmax(action_values)\n",
    "        \n",
    "    def ExploreAction(self):\n",
    "        return np.random.choice(self.action_num,1)[0]\n",
    "    \n",
    "    def AddExperience(self, s, a, r, s2, episode_over):\n",
    "        self.experiences.append((s, a, r, s2, episode_over))\n",
    "    \n",
    "    def UpdateQ(self):\n",
    "        # Update not only current state but randomly\n",
    "        # selected experiences to replay.\n",
    "        experience_replay = random.sample(self.memory, self.batch-1)\n",
    "        \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PlayEpisode(env, agent, epsilon):\n",
    "    # Reset playing environment.\n",
    "    s_t0 = env.reset()\n",
    "    \n",
    "    total_episode_reward = 0\n",
    "    time_steps = 0\n",
    "    episode_over = False\n",
    "    \n",
    "    while (not episode_over):\n",
    "        # Determine whether to explore or exploit.\n",
    "        if (np.random.random() < epsilon):\n",
    "            # Explore.\n",
    "            a_t0 = agent.ExploreAction()\n",
    "        else:\n",
    "            # Exploit.\n",
    "            a_t0 = agent.ExploitAction(s_t0)\n",
    "        \n",
    "        # Perform action and move to next state.\n",
    "        s_t1, reward, episode_over, info = env.step(a_t0)\n",
    "        time_steps += 1\n",
    "        \n",
    "        total_episode_reward += reward\n",
    "        \n",
    "        if episode_over and (time_steps < 500):\n",
    "            reward -= 300\n",
    "            \n",
    "        G = reward + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([(2, 3, 4, 5), (3, 4, 5, 6)], maxlen=2)\n",
      "[(3, 4, 5, 6)]\n",
      "3 4 5 6\n",
      "\n",
      "\n",
      "[(3, 4, 5, 6), (3, 4, 5, 6)]\n"
     ]
    }
   ],
   "source": [
    "mem = deque(maxlen=2)\n",
    "\n",
    "mem.append((1, 2, 3, 4))\n",
    "mem.append((2, 3, 4, 5))\n",
    "mem.append((3, 4, 5, 6))\n",
    "\n",
    "print(mem)\n",
    "\n",
    "minibatch = random.sample(mem, 1)\n",
    "print(minibatch)\n",
    "\n",
    "for i, j, k, l in minibatch:\n",
    "    print(i, j, k, l)\n",
    "    \n",
    "print(\"\\n\")\n",
    "mem[-1]\n",
    "minibatch.append(mem[-1])\n",
    "print(minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 4, 5, 6), (3, 4, 5, 6)]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfpy35]",
   "language": "python",
   "name": "conda-env-tfpy35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
