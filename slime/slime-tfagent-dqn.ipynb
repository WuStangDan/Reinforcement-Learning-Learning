{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.9 (default, Jan 26 2021, 15:33:00) \n",
      "[GCC 8.4.0]\n",
      "\n",
      "Tensorflow 2.5.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports.\n",
    "import sys\n",
    "print(sys.version)\n",
    "print()\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow\", tf.__version__)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print()\n",
    "import slimevolleygym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.trajectories import time_step as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from pyvirtualdisplay import Display\n",
    "displayy = Display(visible=0, size=(1, 1))\n",
    "displayy.start()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 90000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 100  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 20  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF Environment around SlimeVolleyball.\n",
    "class TFSlimeVolley(py_environment.PyEnvironment):\n",
    "    def __init__(self):\n",
    "        #self._action_spec = array_spec.BoundedArraySpec(shape=(), dtype=np.int8, name='action', minimum=0, maximum=1)\n",
    "        #self._observation_spec = ts.TimeStep({'discount': array_spec.BoundedArraySpec(\n",
    "        #    shape=(), dtype=np.float32, name='discount', minimum=0, maximum=1),\n",
    "        #                                     'observation': array_spec.BoundedArraySpec(\n",
    "        #    shape=(12,), dtype=np.float32, name='observation', minimum=np.finfo(np.float32).min, maximum=np.finfo(np.float32).max),\n",
    "        #                         'reward': array_spec.ArraySpec(shape=(), dtype=np.float32, name='reward'),\n",
    "        #                         'step_type': array_spec.ArraySpec(shape=(), dtype=np.int32, name='step_type')})\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(12,), dtype=np.float32, minimum=np.finfo(np.float32).min,\n",
    "            maximum=np.finfo(np.float32).max, name='observation')\n",
    "        \n",
    "        self.env = gym.make('SlimeVolley-v0')\n",
    "        self._episode_ended = False\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "    \n",
    "    def _reset(self):\n",
    "        self._episode_ended = False\n",
    "        return ts.restart(np.array(self.env.reset(), dtype=np.float32))\n",
    "    def _step(self, action):\n",
    "        if self._episode_ended:\n",
    "            return self.reset()\n",
    "        action_reshape = np.zeros(3)\n",
    "        action_reshape[action] = 1\n",
    "        observation, reward, episode_done, info = self.env.step(action_reshape)\n",
    "        if episode_done:\n",
    "            self._episode_ended = True\n",
    "            return ts.termination(np.array(observation, dtype=np.float32), reward)\n",
    "        else:\n",
    "            return ts.transition(np.array(observation, dtype=np.float32), reward=reward, discount=1.0)\n",
    "        \n",
    "    \n",
    "a = TFSlimeVolley()\n",
    "tf_env = tf_py_environment.TFPyEnvironment(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_name = 'CartPole-v0'\n",
    "#env = suite_gym.load(env_name)\n",
    "#py_env = tf_py_environment.TFPyEnvironment(env)\n",
    "#py_env.action_spec()\n",
    "#time_spec = py_env.time_step_spec()\n",
    "#py_env.reset()\n",
    "\n",
    "#env = gym.make('SlimeVolley-v0')\n",
    "#print(\"Observation Space \" + str(env.observation_space.shape))\n",
    "#print(\"Action Space \" + str(env.action_space.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.action_space.sample()\n",
    "#env.observation_space.sample()\n",
    "#env.reset()\n",
    "#env.step([1,0,0])\n",
    "#env.current_time_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 12), dtype=float32, numpy=array([[ 1.2  ,  0.15 ,  0.   ,  0.   ,  0.   ,  1.2  , -0.686,  1.724,  1.2  ,  0.15 ,  0.   ,  0.   ]], dtype=float32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})\n"
     ]
    }
   ],
   "source": [
    "train_env = tf_env\n",
    "b = TFSlimeVolley()\n",
    "eval_env = tf_py_environment.TFPyEnvironment(b)\n",
    "#print(train_env.time_step_spec())\n",
    "print(train_env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.validate_py_environment(b, episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Deep Q Network Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Q Deep NN.\n",
    "\n",
    "fc_layer_params = (1600, 800, 400, 200, 100)\n",
    "action_tensor_spec = tensor_spec.from_spec(train_env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "# Define a helper function to create Dense layers configured with the right\n",
    "# activation and kernel initializer.\n",
    "def dense_layer(num_units):\n",
    "  return tf.keras.layers.Dense(\n",
    "      num_units,\n",
    "      activation=tf.keras.activations.relu,\n",
    "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
    "# with `num_actions` units to generate one q_value per available action as\n",
    "# it's output.\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "        minval=-0.03, maxval=0.03),\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter,\n",
    "    target_update_period=20)\n",
    "\n",
    "agent.initialize()\n",
    "\n",
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "\n",
    "# Random policy to collect initial data.\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=1,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "# See also the metrics module for standard implementations of different metrics.\n",
    "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestModel:\n",
    "    def __init__(self, agent):\n",
    "        self.best_model = keras.models.clone_model(agent.model)\n",
    "        self.best_model.set_weights(agent.model.get_weights())\n",
    "        self.state_num = agent.state_num\n",
    "        self.best_score = -10\n",
    "        self.update_index = 0\n",
    "    def UpdateBest(self, agent, eval_score, i):\n",
    "        if (eval_score > self.best_score):\n",
    "            self.best_model.set_weights(agent.model.get_weights())\n",
    "            self.best_score = eval_score\n",
    "            self.update_index = i\n",
    "            print(\"New best!\")\n",
    "    def RunBest(self, state):\n",
    "        state = np.reshape(state, [1, self.state_num])\n",
    "        action = self.best_model(state, training=False)\n",
    "        return np.argmax(action[0])\n",
    "\n",
    "#bmodel = BestModel(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py:382: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n",
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7fe2b6994cf8>\n"
     ]
    }
   ],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n",
    "\n",
    "# This loop is so common in RL, that we provide standard implementations. \n",
    "# For more details see tutorial 4 or the drivers module.\n",
    "# https://github.com/tensorflow/agents/blob/master/docs/tutorials/4_drivers_tutorial.ipynb \n",
    "# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers\n",
    "\n",
    "# For the curious:\n",
    "# Uncomment to peel one of these off and inspect it.\n",
    "# iter(replay_buffer.as_dataset()).next()\n",
    "\n",
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "\n",
    "dataset\n",
    "iterator = iter(dataset)\n",
    "print(iterator)\n",
    "# For the curious:\n",
    "# Uncomment to see what the dataset iterator is feeding to the agent.\n",
    "# Compare this representation of replay data \n",
    "# to the collection of individual trajectories shown earlier.\n",
    "\n",
    "# iterator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7e996fbee042568053fc9f4c42f5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:206: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
      "step = 200: loss = 0.0009486199705861509\n",
      "step = 400: loss = 0.00415068818256259\n",
      "step = 600: loss = 0.0012521957978606224\n",
      "step = 800: loss = 0.012919792905449867\n",
      "step = 1000: loss = 0.013675099238753319\n",
      "step = 1000: Average Return = -4.949999809265137\n",
      "step = 1200: loss = 0.002205835422500968\n",
      "step = 1400: loss = 0.02317243441939354\n",
      "step = 1600: loss = 0.018308544531464577\n",
      "step = 1800: loss = 0.019229987636208534\n",
      "step = 2000: loss = 0.07849456369876862\n",
      "step = 2000: Average Return = -4.699999809265137\n",
      "step = 2200: loss = 0.007777783088386059\n",
      "step = 2400: loss = 0.04006198048591614\n",
      "step = 2600: loss = 0.00982610322535038\n",
      "step = 2800: loss = 0.015280798077583313\n",
      "step = 3000: loss = 0.019058309495449066\n",
      "step = 3000: Average Return = -4.800000190734863\n",
      "step = 3200: loss = 0.01912495493888855\n",
      "step = 3400: loss = 0.06950043141841888\n",
      "step = 3600: loss = 0.11694931983947754\n",
      "step = 3800: loss = 0.04793126881122589\n",
      "step = 4000: loss = 0.3776848614215851\n",
      "step = 4000: Average Return = -4.949999809265137\n",
      "step = 4200: loss = 0.18991844356060028\n",
      "step = 4400: loss = 0.017125049605965614\n",
      "step = 4600: loss = 0.026750540360808372\n",
      "step = 4800: loss = 0.02511790581047535\n",
      "step = 5000: loss = 0.07420949637889862\n",
      "step = 5000: Average Return = -4.75\n",
      "step = 5200: loss = 0.042530689388513565\n",
      "step = 5400: loss = 0.06195693463087082\n",
      "step = 5600: loss = 0.12014522403478622\n",
      "step = 5800: loss = 0.07927583903074265\n",
      "step = 6000: loss = 0.08010470867156982\n",
      "step = 6000: Average Return = -4.900000095367432\n",
      "step = 6200: loss = 0.1150365024805069\n",
      "step = 6400: loss = 0.6938706040382385\n",
      "step = 6600: loss = 0.13726957142353058\n",
      "step = 6800: loss = 0.3455119729042053\n",
      "step = 7000: loss = 0.37588781118392944\n",
      "step = 7000: Average Return = -4.699999809265137\n",
      "step = 7200: loss = 0.11018595099449158\n",
      "step = 7400: loss = 0.3179776668548584\n",
      "step = 7600: loss = 0.07742932438850403\n",
      "step = 7800: loss = 0.08762288093566895\n",
      "step = 8000: loss = 0.11915053427219391\n",
      "step = 8000: Average Return = -4.849999904632568\n",
      "step = 8200: loss = 0.10811525583267212\n",
      "step = 8400: loss = 0.302140474319458\n",
      "step = 8600: loss = 0.4124771058559418\n",
      "step = 8800: loss = 0.13247525691986084\n",
      "step = 9000: loss = 0.37906894087791443\n",
      "step = 9000: Average Return = -5.0\n",
      "step = 9200: loss = 3.789668083190918\n",
      "step = 9400: loss = 0.18497160077095032\n",
      "step = 9600: loss = 4.870710849761963\n",
      "step = 9800: loss = 0.09427987039089203\n",
      "step = 10000: loss = 0.23770146071910858\n",
      "step = 10000: Average Return = -4.900000095367432\n",
      "step = 10200: loss = 2.254915714263916\n",
      "step = 10400: loss = 0.37737253308296204\n",
      "step = 10600: loss = 0.4018999934196472\n",
      "step = 10800: loss = 0.8255952596664429\n",
      "step = 11000: loss = 1.3128926753997803\n",
      "step = 11000: Average Return = -4.949999809265137\n",
      "step = 11200: loss = 0.9423089027404785\n",
      "step = 11400: loss = 0.8086960911750793\n",
      "step = 11600: loss = 0.9897657036781311\n",
      "step = 11800: loss = 1.5807557106018066\n",
      "step = 12000: loss = 14.957368850708008\n",
      "step = 12000: Average Return = -4.900000095367432\n",
      "step = 12200: loss = 1.5553388595581055\n",
      "step = 12400: loss = 1.1826293468475342\n",
      "step = 12600: loss = 1.6928706169128418\n",
      "step = 12800: loss = 2.696475028991699\n",
      "step = 13000: loss = 17.023605346679688\n",
      "step = 13000: Average Return = -5.0\n",
      "step = 13200: loss = 1.1511834859848022\n",
      "step = 13400: loss = 1.3241708278656006\n",
      "step = 13600: loss = 26.799346923828125\n",
      "step = 13800: loss = 2.1992788314819336\n",
      "step = 14000: loss = 3.1672544479370117\n",
      "step = 14000: Average Return = -4.849999904632568\n",
      "step = 14200: loss = 23.87122917175293\n",
      "step = 14400: loss = 14.021462440490723\n",
      "step = 14600: loss = 2.2340879440307617\n",
      "step = 14800: loss = 34.54550552368164\n",
      "step = 15000: loss = 23.474292755126953\n",
      "step = 15000: Average Return = -4.949999809265137\n",
      "step = 15200: loss = 5.346905708312988\n",
      "step = 15400: loss = 9.221879959106445\n",
      "step = 15600: loss = 6.476808547973633\n",
      "step = 15800: loss = 29.33449935913086\n",
      "step = 16000: loss = 92.40352630615234\n",
      "step = 16000: Average Return = -4.849999904632568\n",
      "step = 16200: loss = 37.9920654296875\n",
      "step = 16400: loss = 1.8707001209259033\n",
      "step = 16600: loss = 202.39349365234375\n",
      "step = 16800: loss = 9.385271072387695\n",
      "step = 17000: loss = 49.96929168701172\n",
      "step = 17000: Average Return = -4.75\n",
      "step = 17200: loss = 13.412582397460938\n",
      "step = 17400: loss = 16.878259658813477\n",
      "step = 17600: loss = 3.8949649333953857\n",
      "step = 17800: loss = 9.347257614135742\n",
      "step = 18000: loss = 8.33558464050293\n",
      "step = 18000: Average Return = -4.949999809265137\n",
      "step = 18200: loss = 7.368451118469238\n",
      "step = 18400: loss = 6.59027099609375\n",
      "step = 18600: loss = 28.24419593811035\n",
      "step = 18800: loss = 6.543188095092773\n",
      "step = 19000: loss = 58.29678726196289\n",
      "step = 19000: Average Return = -4.949999809265137\n",
      "step = 19200: loss = 10.195676803588867\n",
      "step = 19400: loss = 24.823827743530273\n",
      "step = 19600: loss = 54.74378204345703\n",
      "step = 19800: loss = 26.82386016845703\n",
      "step = 20000: loss = 10.064361572265625\n",
      "step = 20000: Average Return = -4.849999904632568\n",
      "step = 20200: loss = 10.087827682495117\n",
      "step = 20400: loss = 27.993240356445312\n",
      "step = 20600: loss = 30.502883911132812\n",
      "step = 20800: loss = 125.0690689086914\n",
      "step = 21000: loss = 213.47857666015625\n",
      "step = 21000: Average Return = -4.75\n",
      "step = 21200: loss = 14.012222290039062\n",
      "step = 21400: loss = 494.6813659667969\n",
      "step = 21600: loss = 185.4017333984375\n",
      "step = 21800: loss = 11.094329833984375\n",
      "step = 22000: loss = 19.611942291259766\n",
      "step = 22000: Average Return = -4.900000095367432\n",
      "step = 22200: loss = 14.332021713256836\n",
      "step = 22400: loss = 131.80355834960938\n",
      "step = 22600: loss = 9.474392890930176\n",
      "step = 22800: loss = 655.3870849609375\n",
      "step = 23000: loss = 207.80657958984375\n",
      "step = 23000: Average Return = -4.849999904632568\n",
      "step = 23200: loss = 10.502195358276367\n",
      "step = 23400: loss = 267.9892272949219\n",
      "step = 23600: loss = 35.355491638183594\n",
      "step = 23800: loss = 317.1313171386719\n",
      "step = 24000: loss = 53.744789123535156\n",
      "step = 24000: Average Return = -5.0\n",
      "step = 24200: loss = 27.025039672851562\n",
      "step = 24400: loss = 56.817466735839844\n",
      "step = 24600: loss = 44.28492736816406\n",
      "step = 24800: loss = 36.226043701171875\n",
      "step = 25000: loss = 449.4492492675781\n",
      "step = 25000: Average Return = -4.949999809265137\n",
      "step = 25200: loss = 373.1838684082031\n",
      "step = 25400: loss = 50.920684814453125\n",
      "step = 25600: loss = 45.77700424194336\n",
      "step = 25800: loss = 1190.5548095703125\n",
      "step = 26000: loss = 1237.093017578125\n",
      "step = 26000: Average Return = -4.949999809265137\n",
      "step = 26200: loss = 70.1983642578125\n",
      "step = 26400: loss = 70.26908874511719\n",
      "step = 26600: loss = 113.2833251953125\n",
      "step = 26800: loss = 73.12279510498047\n",
      "step = 27000: loss = 31.551206588745117\n",
      "step = 27000: Average Return = -4.949999809265137\n",
      "step = 27200: loss = 56.92402267456055\n",
      "step = 27400: loss = 233.50555419921875\n",
      "step = 27600: loss = 797.1578369140625\n",
      "step = 27800: loss = 156.578857421875\n",
      "step = 28000: loss = 161.06626892089844\n",
      "step = 28000: Average Return = -4.75\n",
      "step = 28200: loss = 160.19732666015625\n",
      "step = 28400: loss = 2361.248046875\n",
      "step = 28600: loss = 287.3192138671875\n",
      "step = 28800: loss = 59.842647552490234\n",
      "step = 29000: loss = 2335.141357421875\n",
      "step = 29000: Average Return = -4.949999809265137\n",
      "step = 29200: loss = 268.07110595703125\n",
      "step = 29400: loss = 120.9197998046875\n",
      "step = 29600: loss = 272.5272521972656\n",
      "step = 29800: loss = 689.7591552734375\n",
      "step = 30000: loss = 333.0172119140625\n",
      "step = 30000: Average Return = -4.650000095367432\n",
      "step = 30200: loss = 370.82568359375\n",
      "step = 30400: loss = 114.1688232421875\n",
      "step = 30600: loss = 156.1652069091797\n",
      "step = 30800: loss = 92.37846374511719\n",
      "step = 31000: loss = 123.05654907226562\n",
      "step = 31000: Average Return = -4.849999904632568\n",
      "step = 31200: loss = 217.92642211914062\n",
      "step = 31400: loss = 484.3199768066406\n",
      "step = 31600: loss = 434.84771728515625\n",
      "step = 31800: loss = 193.03964233398438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 32000: loss = 192.90353393554688\n",
      "step = 32000: Average Return = -4.949999809265137\n",
      "step = 32200: loss = 745.7275390625\n",
      "step = 32400: loss = 503.27569580078125\n",
      "step = 32600: loss = 483.1691589355469\n",
      "step = 32800: loss = 652.2904052734375\n",
      "step = 33000: loss = 649.485107421875\n",
      "step = 33000: Average Return = -4.849999904632568\n",
      "step = 33200: loss = 260.25640869140625\n",
      "step = 33400: loss = 889.928955078125\n",
      "step = 33600: loss = 181.34042358398438\n",
      "step = 33800: loss = 397.6488342285156\n",
      "step = 34000: loss = 266.4290771484375\n",
      "step = 34000: Average Return = -4.800000190734863\n",
      "step = 34200: loss = 1861.1512451171875\n",
      "step = 34400: loss = 2875.189697265625\n",
      "step = 34600: loss = 418.78741455078125\n",
      "step = 34800: loss = 366.6003723144531\n",
      "step = 35000: loss = 3285.595458984375\n",
      "step = 35000: Average Return = -4.699999809265137\n",
      "step = 35200: loss = 1659.505126953125\n",
      "step = 35400: loss = 310.13037109375\n",
      "step = 35600: loss = 3311.048095703125\n",
      "step = 35800: loss = 279.54876708984375\n",
      "step = 36000: loss = 1273.4510498046875\n",
      "step = 36000: Average Return = -4.800000190734863\n",
      "step = 36200: loss = 376.6450500488281\n",
      "step = 36400: loss = 1272.595458984375\n",
      "step = 36600: loss = 570.5028686523438\n",
      "step = 36800: loss = 591.3221435546875\n",
      "step = 37000: loss = 1425.7987060546875\n",
      "step = 37000: Average Return = -4.949999809265137\n",
      "step = 37200: loss = 628.3929443359375\n",
      "step = 37400: loss = 3757.99609375\n",
      "step = 37600: loss = 1212.985595703125\n",
      "step = 37800: loss = 390.15478515625\n",
      "step = 38000: loss = 1864.298583984375\n",
      "step = 38000: Average Return = -4.75\n",
      "step = 38200: loss = 369.2764587402344\n",
      "step = 38400: loss = 715.77734375\n",
      "step = 38600: loss = 2393.0712890625\n",
      "step = 38800: loss = 1436.92236328125\n",
      "step = 39000: loss = 988.0883178710938\n",
      "step = 39000: Average Return = -4.849999904632568\n",
      "step = 39200: loss = 1259.103271484375\n",
      "step = 39400: loss = 1167.5506591796875\n",
      "step = 39600: loss = 522.4703369140625\n",
      "step = 39800: loss = 571.3673095703125\n",
      "step = 40000: loss = 1034.4775390625\n",
      "step = 40000: Average Return = -4.949999809265137\n",
      "step = 40200: loss = 761.6260986328125\n",
      "step = 40400: loss = 6652.2158203125\n",
      "step = 40600: loss = 846.2327880859375\n",
      "step = 40800: loss = 922.4417724609375\n",
      "step = 41000: loss = 1108.270751953125\n",
      "step = 41000: Average Return = -4.849999904632568\n",
      "step = 41200: loss = 4305.513671875\n",
      "step = 41400: loss = 991.8228759765625\n",
      "step = 41600: loss = 819.8841552734375\n",
      "step = 41800: loss = 917.73974609375\n",
      "step = 42000: loss = 4011.38671875\n",
      "step = 42000: Average Return = -4.800000190734863\n",
      "step = 42200: loss = 1803.47265625\n",
      "step = 42400: loss = 23296.619140625\n",
      "step = 42600: loss = 3773.967529296875\n",
      "step = 42800: loss = 971.3060302734375\n",
      "step = 43000: loss = 2640.33203125\n",
      "step = 43000: Average Return = -4.949999809265137\n",
      "step = 43200: loss = 37009.265625\n",
      "step = 43400: loss = 7158.0439453125\n",
      "step = 43600: loss = 4673.201171875\n",
      "step = 43800: loss = 2758.09619140625\n",
      "step = 44000: loss = 10091.1044921875\n",
      "step = 44000: Average Return = -4.75\n",
      "step = 44200: loss = 1936.17529296875\n",
      "step = 44400: loss = 1521.4833984375\n",
      "step = 44600: loss = 41049.86328125\n",
      "step = 44800: loss = 3335.197265625\n",
      "step = 45000: loss = 46762.30078125\n",
      "step = 45000: Average Return = -4.75\n",
      "step = 45200: loss = 7745.8818359375\n",
      "step = 45400: loss = 2432.321533203125\n",
      "step = 45600: loss = 2441.13818359375\n",
      "step = 45800: loss = 24231.8125\n",
      "step = 46000: loss = 104928.0390625\n",
      "step = 46000: Average Return = -4.800000190734863\n",
      "step = 46200: loss = 16186.365234375\n",
      "step = 46400: loss = 5181.638671875\n",
      "step = 46600: loss = 8372.0361328125\n",
      "step = 46800: loss = 2483.27197265625\n",
      "step = 47000: loss = 10301.6103515625\n",
      "step = 47000: Average Return = -4.849999904632568\n",
      "step = 47200: loss = 13223.634765625\n",
      "step = 47400: loss = 4651.5341796875\n",
      "step = 47600: loss = 11913.103515625\n",
      "step = 47800: loss = 7078.31591796875\n",
      "step = 48000: loss = 4546.4365234375\n",
      "step = 48000: Average Return = -4.900000095367432\n",
      "step = 48200: loss = 19451.6015625\n",
      "step = 48400: loss = 3089.9892578125\n",
      "step = 48600: loss = 44268.671875\n",
      "step = 48800: loss = 20426.392578125\n",
      "step = 49000: loss = 19685.80859375\n",
      "step = 49000: Average Return = -4.75\n",
      "step = 49200: loss = 7349.142578125\n",
      "step = 49400: loss = 6680.7392578125\n",
      "step = 49600: loss = 4273.02734375\n",
      "step = 49800: loss = 12965.95703125\n",
      "step = 50000: loss = 3836.169677734375\n",
      "step = 50000: Average Return = -4.900000095367432\n",
      "step = 50200: loss = 10394.453125\n",
      "step = 50400: loss = 6984.16015625\n",
      "step = 50600: loss = 14387.0654296875\n",
      "step = 50800: loss = 235495.484375\n",
      "step = 51000: loss = 11821.7109375\n",
      "step = 51000: Average Return = -4.849999904632568\n",
      "step = 51200: loss = 2665.383056640625\n",
      "step = 51400: loss = 5874.7294921875\n",
      "step = 51600: loss = 118852.5859375\n",
      "step = 51800: loss = 6397.9580078125\n",
      "step = 52000: loss = 534413.3125\n",
      "step = 52000: Average Return = -4.800000190734863\n",
      "step = 52200: loss = 20532.259765625\n",
      "step = 52400: loss = 17327.23046875\n",
      "step = 52600: loss = 23158.046875\n",
      "step = 52800: loss = 27265.96875\n",
      "step = 53000: loss = 9736.4462890625\n",
      "step = 53000: Average Return = -4.900000095367432\n",
      "step = 53200: loss = 28062.390625\n",
      "step = 53400: loss = 10784.7255859375\n",
      "step = 53600: loss = 20896.931640625\n",
      "step = 53800: loss = 27900.66015625\n",
      "step = 54000: loss = 9985.421875\n",
      "step = 54000: Average Return = -5.0\n",
      "step = 54200: loss = 16422.50390625\n",
      "step = 54400: loss = 14224.8896484375\n",
      "step = 54600: loss = 16672.26171875\n",
      "step = 54800: loss = 27273.103515625\n",
      "step = 55000: loss = 62572.5078125\n",
      "step = 55000: Average Return = -4.900000095367432\n",
      "step = 55200: loss = 448142.84375\n",
      "step = 55400: loss = 32179.234375\n",
      "step = 55600: loss = 113974.5703125\n",
      "step = 55800: loss = 177979.515625\n",
      "step = 56000: loss = 49703.546875\n",
      "step = 56000: Average Return = -4.849999904632568\n",
      "step = 56200: loss = 44724.859375\n",
      "step = 56400: loss = 30205.41796875\n",
      "step = 56600: loss = 182489.421875\n",
      "step = 56800: loss = 360112.34375\n",
      "step = 57000: loss = 215996.90625\n",
      "step = 57000: Average Return = -4.849999904632568\n",
      "step = 57200: loss = 210189.5\n",
      "step = 57400: loss = 68384.96875\n",
      "step = 57600: loss = 50324.91015625\n",
      "step = 57800: loss = 73229.484375\n",
      "step = 58000: loss = 49070.578125\n",
      "step = 58000: Average Return = -5.0\n",
      "step = 58200: loss = 44163.921875\n",
      "step = 58400: loss = 258234.46875\n",
      "step = 58600: loss = 23398.52734375\n",
      "step = 58800: loss = 32031.7578125\n",
      "step = 59000: loss = 231630.546875\n",
      "step = 59000: Average Return = -4.949999809265137\n",
      "step = 59200: loss = 35817.16796875\n",
      "step = 59400: loss = 398570.0625\n",
      "step = 59600: loss = 3908914.5\n",
      "step = 59800: loss = 75211.078125\n",
      "step = 60000: loss = 85729.078125\n",
      "step = 60000: Average Return = -4.900000095367432\n",
      "step = 60200: loss = 43381.765625\n",
      "step = 60400: loss = 97193.8359375\n",
      "step = 60600: loss = 34990.1640625\n",
      "step = 60800: loss = 47267.171875\n",
      "step = 61000: loss = 121531.5703125\n",
      "step = 61000: Average Return = -4.699999809265137\n",
      "step = 61200: loss = 489787.78125\n",
      "step = 61400: loss = 116551.2578125\n",
      "step = 61600: loss = 214090.46875\n",
      "step = 61800: loss = 105635.5703125\n",
      "step = 62000: loss = 176134.546875\n",
      "step = 62000: Average Return = -4.900000095367432\n",
      "step = 62200: loss = 349709.1875\n",
      "step = 62400: loss = 83662.2265625\n",
      "step = 62600: loss = 1224721.875\n",
      "step = 62800: loss = 3319274.5\n",
      "step = 63000: loss = 167447.828125\n",
      "step = 63000: Average Return = -5.0\n",
      "step = 63200: loss = 538153.5625\n",
      "step = 63400: loss = 128304.546875\n",
      "step = 63600: loss = 2639988.0\n",
      "step = 63800: loss = 326958.84375\n",
      "step = 64000: loss = 175967.59375\n",
      "step = 64000: Average Return = -5.0\n",
      "step = 64200: loss = 501394.28125\n",
      "step = 64400: loss = 117656.5234375\n",
      "step = 64600: loss = 266130.53125\n",
      "step = 64800: loss = 3841262.75\n",
      "step = 65000: loss = 11029754.0\n",
      "step = 65000: Average Return = -4.949999809265137\n",
      "step = 65200: loss = 495446.71875\n",
      "step = 65400: loss = 972521.25\n",
      "step = 65600: loss = 350101.875\n",
      "step = 65800: loss = 1372595.25\n",
      "step = 66000: loss = 427245.625\n",
      "step = 66000: Average Return = -4.949999809265137\n",
      "step = 66200: loss = 381328.625\n",
      "step = 66400: loss = 1073837.5\n",
      "step = 66600: loss = 3324581.5\n",
      "step = 66800: loss = 236878.578125\n",
      "step = 67000: loss = 186565.3125\n",
      "step = 67000: Average Return = -5.0\n",
      "step = 67200: loss = 364668.125\n",
      "step = 67400: loss = 274200.375\n",
      "step = 67600: loss = 751145.1875\n",
      "step = 67800: loss = 850646.25\n",
      "step = 68000: loss = 170266.453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 68000: Average Return = -4.849999904632568\n",
      "step = 68200: loss = 370059.84375\n",
      "step = 68400: loss = 327373.03125\n",
      "step = 68600: loss = 3755084.75\n",
      "step = 68800: loss = 94964.4453125\n",
      "step = 69000: loss = 189866.09375\n",
      "step = 69000: Average Return = -4.849999904632568\n",
      "step = 69200: loss = 875707.375\n",
      "step = 69400: loss = 181983.8125\n",
      "step = 69600: loss = 11191054.0\n",
      "step = 69800: loss = 7036163.0\n",
      "step = 70000: loss = 1047173.9375\n",
      "step = 70000: Average Return = -4.900000095367432\n",
      "step = 70200: loss = 504424.9375\n",
      "step = 70400: loss = 1022830.25\n",
      "step = 70600: loss = 3924122.0\n",
      "step = 70800: loss = 323931.46875\n",
      "step = 71000: loss = 499378.5625\n",
      "step = 71000: Average Return = -4.699999809265137\n",
      "step = 71200: loss = 507634.0625\n",
      "step = 71400: loss = 261663.34375\n",
      "step = 71600: loss = 888309.0625\n",
      "step = 71800: loss = 758320.375\n",
      "step = 72000: loss = 4632989.5\n",
      "step = 72000: Average Return = -4.949999809265137\n",
      "step = 72200: loss = 383958.8125\n",
      "step = 72400: loss = 437921.15625\n",
      "step = 72600: loss = 373301.0\n",
      "step = 72800: loss = 767823.8125\n",
      "step = 73000: loss = 1032761.25\n",
      "step = 73000: Average Return = -4.849999904632568\n",
      "step = 73200: loss = 1301384.875\n",
      "step = 73400: loss = 1908987.0\n",
      "step = 73600: loss = 3269486.0\n",
      "step = 73800: loss = 20418816.0\n",
      "step = 74000: loss = 718707.75\n",
      "step = 74000: Average Return = -5.0\n",
      "step = 74200: loss = 2607701.5\n",
      "step = 74400: loss = 3204712.0\n",
      "step = 74600: loss = 617954.4375\n",
      "step = 74800: loss = 1026071.0\n",
      "step = 75000: loss = 746828.3125\n",
      "step = 75000: Average Return = -4.550000190734863\n",
      "step = 75200: loss = 1277952.125\n",
      "step = 75400: loss = 3112452.75\n",
      "step = 75600: loss = 4303013.5\n",
      "step = 75800: loss = 2232561.0\n",
      "step = 76000: loss = 699502.9375\n",
      "step = 76000: Average Return = -4.900000095367432\n",
      "step = 76200: loss = 933211.5\n",
      "step = 76400: loss = 415854.8125\n",
      "step = 76600: loss = 12604915.0\n",
      "step = 76800: loss = 1011129.6875\n",
      "step = 77000: loss = 1138100.625\n",
      "step = 77000: Average Return = -4.949999809265137\n",
      "step = 77200: loss = 990767.25\n",
      "step = 77400: loss = 6880214.5\n",
      "step = 77600: loss = 5767336.0\n",
      "step = 77800: loss = 18342838.0\n",
      "step = 78000: loss = 2457348.5\n",
      "step = 78000: Average Return = -4.900000095367432\n",
      "step = 78200: loss = 40219456.0\n",
      "step = 78400: loss = 2297973.5\n",
      "step = 78600: loss = 5153001.5\n",
      "step = 78800: loss = 6405452.0\n",
      "step = 79000: loss = 642837.25\n",
      "step = 79000: Average Return = -4.900000095367432\n",
      "step = 79200: loss = 1224405.25\n",
      "step = 79400: loss = 1722814.125\n",
      "step = 79600: loss = 4468237.5\n",
      "step = 79800: loss = 2713872.0\n",
      "step = 80000: loss = 4294857.5\n",
      "step = 80000: Average Return = -5.0\n",
      "step = 80200: loss = 2937905.0\n",
      "step = 80400: loss = 1246710.75\n",
      "step = 80600: loss = 3201664.0\n",
      "step = 80800: loss = 4057637.0\n",
      "step = 81000: loss = 3665457.0\n",
      "step = 81000: Average Return = -4.800000190734863\n",
      "step = 81200: loss = 1174724.0\n",
      "step = 81400: loss = 3902033.0\n",
      "step = 81600: loss = 3680813.75\n",
      "step = 81800: loss = 8443440.0\n",
      "step = 82000: loss = 55233988.0\n",
      "step = 82000: Average Return = -4.849999904632568\n",
      "step = 82200: loss = 136573488.0\n",
      "step = 82400: loss = 4952775.0\n",
      "step = 82600: loss = 1837351.375\n",
      "step = 82800: loss = 20280876.0\n",
      "step = 83000: loss = 9296849.0\n",
      "step = 83000: Average Return = -4.849999904632568\n",
      "step = 83200: loss = 7832251.5\n",
      "step = 83400: loss = 108862536.0\n",
      "step = 83600: loss = 1419957.375\n",
      "step = 83800: loss = 1150586.125\n",
      "step = 84000: loss = 3546886.75\n",
      "step = 84000: Average Return = -4.900000095367432\n",
      "step = 84200: loss = 3326914.25\n",
      "step = 84400: loss = 2771071.0\n",
      "step = 84600: loss = 3984129.5\n",
      "step = 84800: loss = 2314308.0\n",
      "step = 85000: loss = 3550619.0\n",
      "step = 85000: Average Return = -4.900000095367432\n",
      "step = 85200: loss = 6232719.5\n",
      "step = 85400: loss = 10892727.0\n",
      "step = 85600: loss = 14933782.0\n",
      "step = 85800: loss = 5926921.0\n",
      "step = 86000: loss = 31228208.0\n",
      "step = 86000: Average Return = -4.900000095367432\n",
      "step = 86200: loss = 6146874.5\n",
      "step = 86400: loss = 8051200.5\n",
      "step = 86600: loss = 66439304.0\n",
      "step = 86800: loss = 83237112.0\n",
      "step = 87000: loss = 126603120.0\n",
      "step = 87000: Average Return = -4.949999809265137\n",
      "step = 87200: loss = 6868787.5\n",
      "step = 87400: loss = 5274777.0\n",
      "step = 87600: loss = 1897429.625\n",
      "step = 87800: loss = 33286688.0\n",
      "step = 88000: loss = 33571624.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-24b7a171837e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mavg_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_avg_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_eval_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'step = {0}: Average Return = {1}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mreturns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-82c653582342>\u001b[0m in \u001b[0;36mcompute_avg_return\u001b[0;34m(environment, policy, num_episodes)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0maction_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m       \u001b[0mtime_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mepisode_return\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36maction\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclip_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tf_agents/utils/common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36m_action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    561\u001b[0m     actions = tf.nest.map_structure(\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreparameterized_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m         distribution_step.action)\n\u001b[0m\u001b[1;32m    564\u001b[0m     \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memit_log_probability\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0mdistribution_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=wrong-arg-types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m     actions = tf.nest.map_structure(\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreparameterized_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         distribution_step.action)\n\u001b[1;32m    564\u001b[0m     \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tf_agents/distributions/reparameterized_sampling.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(distribution, reparam, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/distributions/distribution.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sample_shape, seed, name, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m       \u001b[0msamples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mprepended\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m     \"\"\"\n\u001b[0;32m-> 1002\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_sample_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/distributions/distribution.py\u001b[0m in \u001b[0;36m_call_sample_n\u001b[0;34m(self, sample_shape, seed, name, **kwargs)\u001b[0m\n\u001b[1;32m    978\u001b[0m           sample_shape, 'sample_shape')\n\u001b[1;32m    979\u001b[0m       samples = self._sample_n(\n\u001b[0;32m--> 980\u001b[0;31m           n, seed=seed() if callable(seed) else seed, **kwargs)\n\u001b[0m\u001b[1;32m    981\u001b[0m       \u001b[0mbatch_event_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m       \u001b[0mfinal_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_event_shape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/distributions/deterministic.py\u001b[0m in \u001b[0;36m_sample_n\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    151\u001b[0m     return tf.broadcast_to(\n\u001b[1;32m    152\u001b[0m         \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         ps.concat([[n], self._batch_shape_tensor(loc=loc),\n\u001b[0m\u001b[1;32m    154\u001b[0m                    self._event_shape_tensor(loc=loc)],\n\u001b[1;32m    155\u001b[0m                   axis=0))\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/distributions/deterministic.py\u001b[0m in \u001b[0;36m_batch_shape_tensor\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m    296\u001b[0m     return ps.broadcast_shape(\n\u001b[1;32m    297\u001b[0m         \u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         ps.broadcast_shape(ps.shape(self.atol), ps.shape(self.rtol)))\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_batch_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/decorator.py\u001b[0m in \u001b[0;36mfix\u001b[0;34m(args, kwargs, sig)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0mFix\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mconsistent\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \"\"\"\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m     \u001b[0mba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# needed for test_dan_schult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2995\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mcan\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2996\u001b[0m         \"\"\"\n\u001b[0;32m-> 2997\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2999\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbind_partial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36m_bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   2864\u001b[0m         \u001b[0marguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2866\u001b[0;31m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2867\u001b[0m         \u001b[0mparameters_ex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2868\u001b[0m         \u001b[0marg_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in trange(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=5.5, bottom=-5.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watching a Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "while not done:\n",
    "    action = bmodel.RunBest(obs)\n",
    "    action_reshaped = np.zeros(agent.action_num)\n",
    "    action_reshaped[action] = 1\n",
    "    obs, reward, done, info = env.step(action_reshaped)\n",
    "    total_reward += reward\n",
    "    img.set_data(env.render(mode='rgb_array'))\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "print(\"score:\", total_reward)\n",
    "print(bmodel.update_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
