{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.9 (default, Jan 26 2021, 15:33:00) \n",
      "[GCC 8.4.0]\n",
      "\n",
      "Tensorflow 2.5.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports.\n",
    "import sys\n",
    "print(sys.version)\n",
    "print()\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow\", tf.__version__)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print()\n",
    "import slimevolleygym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.networks import actor_distribution_rnn_network\n",
    "from tf_agents.networks import value_network\n",
    "from tf_agents.networks import value_rnn_network\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "from tf_agents.agents import PPOClipAgent\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.trajectories import time_step as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from pyvirtualdisplay import Display\n",
    "displayy = Display(visible=0, size=(1, 1))\n",
    "displayy.start()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 100  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF Environment around SlimeVolleyball.\n",
    "class TFSlimeVolley(py_environment.PyEnvironment):\n",
    "    def __init__(self):\n",
    "        #self._action_spec = array_spec.BoundedArraySpec(shape=(), dtype=np.int8, name='action', minimum=0, maximum=1)\n",
    "        #self._observation_spec = ts.TimeStep({'discount': array_spec.BoundedArraySpec(\n",
    "        #    shape=(), dtype=np.float32, name='discount', minimum=0, maximum=1),\n",
    "        #                                     'observation': array_spec.BoundedArraySpec(\n",
    "        #    shape=(12,), dtype=np.float32, name='observation', minimum=np.finfo(np.float32).min, maximum=np.finfo(np.float32).max),\n",
    "        #                         'reward': array_spec.ArraySpec(shape=(), dtype=np.float32, name='reward'),\n",
    "        #                         'step_type': array_spec.ArraySpec(shape=(), dtype=np.int32, name='step_type')})\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(12,), dtype=np.float32, minimum=np.finfo(np.float32).min,\n",
    "            maximum=np.finfo(np.float32).max, name='observation')\n",
    "        \n",
    "        self.env = gym.make('SlimeVolley-v0')\n",
    "        self._episode_ended = False\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "    \n",
    "    def _reset(self):\n",
    "        self._episode_ended = False\n",
    "        return ts.restart(np.array(self.env.reset(), dtype=np.float32))\n",
    "    def _step(self, action):\n",
    "        if self._episode_ended:\n",
    "            return self.reset()\n",
    "        action_reshape = np.zeros(3)\n",
    "        action_reshape[action] = 1\n",
    "        observation, reward, episode_done, info = self.env.step(action_reshape)\n",
    "        if episode_done:\n",
    "            self._episode_ended = True\n",
    "            return ts.termination(np.array(observation, dtype=np.float32), reward)\n",
    "        else:\n",
    "            return ts.transition(np.array(observation, dtype=np.float32), reward=reward, discount=1.0)\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_name = 'CartPole-v0'\n",
    "#env = suite_gym.load(env_name)\n",
    "#py_env = tf_py_environment.TFPyEnvironment(env)\n",
    "#py_env.action_spec()\n",
    "#time_spec = py_env.time_step_spec()\n",
    "#py_env.reset()\n",
    "\n",
    "#env = gym.make('SlimeVolley-v0')\n",
    "#print(\"Observation Space \" + str(env.observation_space.shape))\n",
    "#print(\"Action Space \" + str(env.action_space.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.action_space.sample()\n",
    "#env.observation_space.sample()\n",
    "#env.reset()\n",
    "#env.step([1,0,0])\n",
    "#env.current_time_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(TFSlimeVolley())\n",
    "eval_env = tf_py_environment.TFPyEnvironment(TFSlimeVolley())\n",
    "#print(train_env.time_step_spec())\n",
    "#print(train_env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.validate_py_environment(TFSlimeVolley(), episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(2, dtype=int32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env.action_spec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Create Actor and Value net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Actor and Value net.\n",
    "fc_layer_params = (200, 100)\n",
    "actor_fc_layers = (200, 100)\n",
    "value_fc_layers = (200, 100)\n",
    "lstm_size=(20,)\n",
    "\n",
    "# actor_net = actor_distribution_rnn_network.ActorDistributionRnnNetwork(\n",
    "#   tf_env.observation_spec(),\n",
    "#   tf_env.action_spec(),\n",
    "#   input_fc_layer_params=actor_fc_layers,\n",
    "#   output_fc_layer_params=None,\n",
    "#   lstm_size=lstm_size)\n",
    "# value_net = value_rnn_network.ValueRnnNetwork(\n",
    "#   tf_env.observation_spec(),\n",
    "#   input_fc_layer_params=value_fc_layers,\n",
    "#   output_fc_layer_params=None)\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "  train_env.observation_spec(),\n",
    "  train_env.action_spec(),\n",
    "  fc_layer_params=actor_fc_layers,\n",
    "  activation_fn=tf.keras.activations.tanh)\n",
    "value_net = value_network.ValueNetwork(\n",
    "  train_env.observation_spec(),\n",
    "  fc_layer_params=value_fc_layers,\n",
    "  activation_fn=tf.keras.activations.tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = PPOClipAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    actor_net=actor_net,\n",
    "    value_net=value_net,\n",
    "    optimizer=optimizer,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n",
    "\n",
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "\n",
    "# Random policy to collect initial data.\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=1,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "# See also the metrics module for standard implementations of different metrics.\n",
    "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class BestModel:\n",
    "    def __init__(self, agent):\n",
    "        self.best_model = keras.models.clone_model(agent.model)\n",
    "        self.best_model.set_weights(agent.model.get_weights())\n",
    "        self.state_num = agent.state_num\n",
    "        self.best_score = -10\n",
    "        self.update_index = 0\n",
    "    def UpdateBest(self, agent, eval_score, i):\n",
    "        if (eval_score > self.best_score):\n",
    "            self.best_model.set_weights(agent.model.get_weights())\n",
    "            self.best_score = eval_score\n",
    "            self.update_index = i\n",
    "            print(\"New best!\")\n",
    "    def RunBest(self, state):\n",
    "        state = np.reshape(state, [1, self.state_num])\n",
    "        action = self.best_model(state, training=False)\n",
    "        return np.argmax(action[0])\n",
    "\n",
    "#bmodel = BestModel(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The two structures do not match:\n  Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': (),\n 'reward': .,\n 'step_type': .})\nvs.\n  Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': DictWrapper({'dist_params': DictWrapper({'logits': .})}),\n 'reward': .,\n 'step_type': .})\nValues:\n  Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 12), dtype=float32, numpy=array([[ 1.2  ,  0.15 ,  0.   ,  0.   ,  0.   ,  1.2  , -1.182,  1.94 ,  1.2  ,  0.15 ,  0.   ,  0.   ]], dtype=float32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})\nvs.\n  Trajectory(\n{'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(2, dtype=int32)),\n 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n 'observation': BoundedTensorSpec(shape=(12,), dtype=tf.float32, name='observation', minimum=array(-3.403e+38, dtype=float32), maximum=array(3.403e+38, dtype=float32)),\n 'policy_info': {'dist_params': {'logits': TensorSpec(shape=(3,), dtype=tf.float32, name='CategoricalProjectionNetwork_logits')}},\n 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7e69d38c8414>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcollect_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mcollect_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_collect_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# This loop is so common in RL, that we provide standard implementations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-7e69d38c8414>\u001b[0m in \u001b[0;36mcollect_data\u001b[0;34m(env, policy, buffer, steps)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcollect_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mcollect_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mcollect_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_collect_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-7e69d38c8414>\u001b[0m in \u001b[0;36mcollect_step\u001b[0;34m(environment, policy, buffer)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# Add trajectory to the replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcollect_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tf_agents/replay_buffers/replay_buffer.py\u001b[0m in \u001b[0;36madd_batch\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m     81\u001b[0m       \u001b[0mAdds\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreplay\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m   @deprecation.deprecated(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tf_agents/replay_buffers/tf_uniform_replay_buffer.py\u001b[0m in \u001b[0;36m_add_batch\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    191\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0monce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0mnest_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0;31m# Calling get_outer_rank here will validate that all items have the same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# outer rank. This was not usually an issue, but now that it's easier to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tf_agents/utils/nest_utils.py\u001b[0m in \u001b[0;36massert_same_structure\u001b[0;34m(nest1, nest2, check_types, expand_composites, allow_shallow_nest1, message)\u001b[0m\n\u001b[1;32m    124\u001b[0m         lambda _: _DOT, nest2, expand_composites=expand_composites)\n\u001b[1;32m    125\u001b[0m     raise exception('{}:\\n  {}\\nvs.\\n  {}\\nValues:\\n  {}\\nvs.\\n  {}.'\n\u001b[0;32m--> 126\u001b[0;31m                     .format(message, str1, str2, nest1, nest2))\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: The two structures do not match:\n  Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': (),\n 'reward': .,\n 'step_type': .})\nvs.\n  Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': DictWrapper({'dist_params': DictWrapper({'logits': .})}),\n 'reward': .,\n 'step_type': .})\nValues:\n  Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 12), dtype=float32, numpy=array([[ 1.2  ,  0.15 ,  0.   ,  0.   ,  0.   ,  1.2  , -1.182,  1.94 ,  1.2  ,  0.15 ,  0.   ,  0.   ]], dtype=float32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})\nvs.\n  Trajectory(\n{'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(2, dtype=int32)),\n 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n 'observation': BoundedTensorSpec(shape=(12,), dtype=tf.float32, name='observation', minimum=array(-3.403e+38, dtype=float32), maximum=array(3.403e+38, dtype=float32)),\n 'policy_info': {'dist_params': {'logits': TensorSpec(shape=(3,), dtype=tf.float32, name='CategoricalProjectionNetwork_logits')}},\n 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})."
     ]
    }
   ],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n",
    "\n",
    "# This loop is so common in RL, that we provide standard implementations. \n",
    "# For more details see tutorial 4 or the drivers module.\n",
    "# https://github.com/tensorflow/agents/blob/master/docs/tutorials/4_drivers_tutorial.ipynb \n",
    "# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers\n",
    "\n",
    "# For the curious:\n",
    "# Uncomment to peel one of these off and inspect it.\n",
    "# iter(replay_buffer.as_dataset()).next()\n",
    "\n",
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "\n",
    "dataset\n",
    "iterator = iter(dataset)\n",
    "print(iterator)\n",
    "# For the curious:\n",
    "# Uncomment to see what the dataset iterator is feeding to the agent.\n",
    "# Compare this representation of replay data \n",
    "# to the collection of individual trajectories shown earlier.\n",
    "\n",
    "# iterator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in trange(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=5.5, bottom=-5.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watching a Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "while not done:\n",
    "    action = bmodel.RunBest(obs)\n",
    "    action_reshaped = np.zeros(agent.action_num)\n",
    "    action_reshaped[action] = 1\n",
    "    obs, reward, done, info = env.step(action_reshaped)\n",
    "    total_reward += reward\n",
    "    img.set_data(env.render(mode='rgb_array'))\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "print(\"score:\", total_reward)\n",
    "print(bmodel.update_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
